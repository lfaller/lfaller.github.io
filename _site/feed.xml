<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-09-04T12:49:29-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Lina L. Faller, PhD</title><subtitle>Welcome to my webpage</subtitle><entry><title type="html">The Art of Saying No (Without Losing Friends)</title><link href="http://localhost:4000/career-development/2025/09/04/the-art-of-saying-no-without-losing-friends.html" rel="alternate" type="text/html" title="The Art of Saying No (Without Losing Friends)" /><published>2025-09-04T09:00:00-04:00</published><updated>2025-09-04T09:00:00-04:00</updated><id>http://localhost:4000/career-development/2025/09/04/the-art-of-saying-no-without-losing-friends</id><content type="html" xml:base="http://localhost:4000/career-development/2025/09/04/the-art-of-saying-no-without-losing-friends.html"><![CDATA[<p><img src="/assets/images/posts/2025-09-04-the-art-of-saying-no-without-losing-friends.png" alt="Lots of distractions along the road to success" /></p>

<p>Three weeks into your project, someone says: “Wouldn’t it be nice if we could also…”</p>

<p>Your heart sinks. You know exactly where this leads. 🚨</p>

<p>Scope creep is the silent killer of technical projects. Here’s how to prevent it:</p>

<p>𝗦𝘁𝗲𝗽 𝟭: 𝗗𝗲𝗳𝗶𝗻𝗲 𝗪𝗵𝗮𝘁 𝗬𝗼𝘂’𝗿𝗲 𝗕𝘂𝗶𝗹𝗱𝗶𝗻𝗴 (𝗮𝗻𝗱 𝗪𝗵𝗮𝘁 𝗬𝗼𝘂’𝗿𝗲 𝗡𝗼𝘁)</p>

<p>Start with clear requirements. But here’s the secret: also create an explicit “Out of Scope” section.</p>

<p>Those nice-to-have features people mention in meetings? Write them down as “Version 2.0 features.” Acknowledge them, document them, but don’t build them.</p>

<p>This serves two purposes:</p>

<p>→ Shows you’re listening to stakeholder ideas</p>

<p>→ Creates a clear boundary for current work</p>

<p>𝗦𝘁𝗲𝗽 𝟮: 𝗚𝗲𝘁 𝗦𝘁𝗮𝗸𝗲𝗵𝗼𝗹𝗱𝗲𝗿 𝗕𝘂𝘆-𝗶𝗻</p>

<p>Remember your stakeholder mapping? Make sure your project plan has something that satisfies each key player.</p>

<p>Get them to explicitly agree to the scope. Don’t just send a document—have a conversation.</p>

<p>“This is what we’re building. This is what we’re not building right now. Are we aligned?”</p>

<p>Get verbal confirmation. Even better, get it in writing.</p>

<p>𝗦𝘁𝗲𝗽 𝟯: 𝗦𝘁𝗮𝘆 𝗼𝗻 𝘁𝗵𝗲 𝗣𝗮𝘁𝗵</p>

<p>Here comes the hard part: during implementation, stick to the plan.</p>

<p>When someone inevitably says “wouldn’t it be nice if…” your response is:</p>

<p>“That’s a great idea! I’m adding it to our Version 2.0 list. Let’s make sure we nail Version 1.0 first.”</p>

<p>𝗧𝗵𝗲 𝗣𝘀𝘆𝗰𝗵𝗼𝗹𝗼𝗴𝘆 𝗼𝗳 𝗦𝗰𝗼𝗽𝗲 𝗖𝗿𝗲𝗲𝗽</p>

<p>Why do stakeholders keep adding features mid-project?</p>

<p>→ They get excited seeing progress</p>

<p>→ They think “just one more thing” won’t hurt</p>

<p>→ They don’t understand the compounding complexity</p>

<p>→ They haven’t felt the pain of scope creep before</p>

<p>Your job is to protect them from themselves while keeping them engaged.</p>

<p>𝗠𝘆 𝗚𝗼-𝗧𝗼 𝗥𝗲𝘀𝗽𝗼𝗻𝘀𝗲:</p>

<p>“I love the enthusiasm! That feature would definitely add value. Here’s what it would cost us in terms of timeline and current scope. Should we adjust our priorities?”</p>

<p>This does three things:</p>

<p>→ Acknowledges their idea positively</p>

<p>→ Makes the trade-off explicit</p>

<p>→ Puts the decision back in their hands</p>

<p>𝗧𝗵𝗲 𝗕𝗿𝗶𝗱𝗴𝗲 𝗕𝘂𝗶𝗹𝗱𝗲𝗿 𝗜𝗻𝘀𝗶𝗴𝗵𝘁:</p>

<p>Scope negotiation isn’t about being rigid—it’s about being intentional. You’re helping stakeholders make informed decisions about trade-offs rather than just saying no.</p>

<p>When you frame scope management as protecting shared success rather than limiting possibilities, stakeholders become your allies in maintaining focus.</p>

<p>What’s your experience with scope creep? What strategies help you keep projects on track?</p>]]></content><author><name>lina</name></author><category term="career-development" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">How to Turn Stakeholders from Obstacles into Advocates</title><link href="http://localhost:4000/career-development/2025/09/02/how-to-turn-stakeholders-from-obstacles-into-advocates.html" rel="alternate" type="text/html" title="How to Turn Stakeholders from Obstacles into Advocates" /><published>2025-09-02T09:00:00-04:00</published><updated>2025-09-02T09:00:00-04:00</updated><id>http://localhost:4000/career-development/2025/09/02/how-to-turn-stakeholders-from-obstacles-into-advocates</id><content type="html" xml:base="http://localhost:4000/career-development/2025/09/02/how-to-turn-stakeholders-from-obstacles-into-advocates.html"><![CDATA[<p><img src="/assets/images/posts/2025-09-02-how-to-turn-stakeholders-from-obstacles-into-advocates.jpg" alt="Connect to your stakeholders will lead to project success" /></p>

<p>You just got assigned a new project. Your first instinct? Dive into the technical requirements.</p>

<p>Wrong move. 🚫</p>

<p>Your first step should be mapping your stakeholders—not just who they are, but what makes them tick.</p>

<p>𝗦𝘁𝗲𝗽 𝟭: 𝗜𝗱𝗲𝗻𝘁𝗶𝗳𝘆 𝗬𝗼𝘂𝗿 𝗦𝘁𝗮𝗸𝗲𝗵𝗼𝗹𝗱𝗲𝗿𝘀</p>

<p>Beyond the obvious project sponsor, look for:</p>

<p>→ The person whose data you’ll need</p>

<p>→ The scientist who’ll actually use what you build</p>

<p>→ The team lead who controls resources</p>

<p>→ The domain expert who can make or break adoption</p>

<p>→ The person who’ll maintain this after you move on</p>

<p>𝗦𝘁𝗲𝗽 𝟮: 𝗨𝗻𝗱𝗲𝗿𝘀𝘁𝗮𝗻𝗱 𝗪𝗵𝗮𝘁 𝗗𝗿𝗶𝘃𝗲𝘀 𝗧𝗵𝗲𝗺</p>

<p>Each stakeholder has different motivations:</p>

<p>🤝 The Collaborator: wants shared wins and team recognition.</p>

<p>🎯 The Individualist: wants personal impact and clear attribution.</p>

<p>🧩 The Problem-Solver: wants intellectual challenge.</p>

<p>📊 The Results-Oriented person: wants measurable outcomes and clear timelines.</p>

<p>𝗦𝘁𝗲𝗽 𝟯: 𝗙𝗶𝗻𝗱 𝘁𝗵𝗲 𝗢𝘃𝗲𝗿𝗹𝗮𝗽</p>

<p>Here’s where you build the bridges: connect project components to stakeholder strengths and motivations.</p>

<p>Data validation → Problem-solver designs checks</p>

<p>User interfaces → Collaborator gathers feedback</p>

<p>Leadership buy-in → Results person frames metrics</p>

<p>𝗦𝘁𝗲𝗽 𝟰: 𝗠𝗮𝗸𝗲 𝗧𝗵𝗲𝗺 𝗟𝗼𝗼𝗸 𝗚𝗼𝗼𝗱</p>

<p>Make every interaction help stakeholders succeed in their roles.</p>

<p>Expert prevents pitfall? Recognize their insight.</p>

<p>Owner provides clean data? Highlight their contribution.</p>

<p>𝗧𝗵𝗲 𝗕𝗿𝗶𝗱𝗴𝗲 𝗕𝘂𝗶𝗹𝗱𝗲𝗿 𝗜𝗻𝘀𝗶𝗴𝗵𝘁:</p>

<p>Stakeholder management is alignment, not manipulation.</p>

<p>When they feel ownership, they become advocates instead of obstacles. ✨</p>

<p>𝗪𝗵𝗮𝘁 𝗧𝗵𝗶𝘀 𝗟𝗼𝗼𝗸𝘀 𝗟𝗶𝗸𝗲:</p>

<p>Instead of: “I need access to the production database.”</p>

<p>Try: “I’d love your input on the safest way to access the data we need. What would make you comfortable with our approach?”</p>

<p>Instead of: “The analysis is taking longer than expected.”</p>

<p>Try: “We discovered some interesting data quality patterns that might impact future projects. Can we schedule time to discuss what we learned?”</p>

<p>𝗧𝗵𝗲 𝗕𝗼𝘁𝘁𝗼𝗺 𝗟𝗶𝗻𝗲:</p>

<p>Technical skills get you the job. Stakeholder management skills make you effective in the job.
Your project’s success depends more on people dynamics than code quality. Plan accordingly. 🎯</p>

<p>What’s your experience with stakeholder management? What strategies have worked (or failed) for you?</p>]]></content><author><name>lina</name></author><category term="career-development" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Where Data Goes to Die (And How to Save It)</title><link href="http://localhost:4000/data-science/2025/08/28/where-data-goes-to-die.html" rel="alternate" type="text/html" title="Where Data Goes to Die (And How to Save It)" /><published>2025-08-28T09:00:00-04:00</published><updated>2025-08-28T09:00:00-04:00</updated><id>http://localhost:4000/data-science/2025/08/28/where-data-goes-to-die</id><content type="html" xml:base="http://localhost:4000/data-science/2025/08/28/where-data-goes-to-die.html"><![CDATA[<p><img src="/assets/images/posts/2025-08-28-where-data-goes-to-die.png" alt="How to organize your data?" /></p>

<p>Your pilot project just got cancelled. The promising drug target didn’t pan out. The exploratory analysis is being shelved.</p>

<p>What happens to all that data you spent months generating? 📊</p>

<p>𝗧𝗵𝗲 𝗰𝗼𝗺𝗺𝗼𝗻 𝘀𝗰𝗲𝗻𝗮𝗿𝗶𝗼:</p>

<p>Data lives on someone’s laptop. Project gets discontinued. Person moves to different project. Data disappears into the digital void.</p>

<p>Sound familiar? 😅</p>

<p>𝗪𝗵𝘆 𝘁𝗵𝗶𝘀 𝗺𝗮𝘁𝘁𝗲𝗿𝘀:</p>

<p>That “failed” pilot might contain insights valuable for future work. The cancelled project might have generated negative results that save someone else months of effort.</p>

<p>But only if you can find it. 🔍</p>

<p>𝗗𝗼𝗰𝘂𝗺𝗲𝗻𝘁 𝗶𝘁 𝗳𝗶𝗿𝘀𝘁:</p>

<p>Before you archive anything, write it down. Create an “engineering report”:</p>

<p>→ Background: What were you trying to solve?</p>

<p>→ Research question: What hypothesis were you testing?</p>

<p>→ Methods: How did you generate this data?</p>

<p>→ Why it ended: What changed or didn’t work?</p>

<p>Future you will thank you. 🙏</p>

<p>𝗪𝗵𝗲𝗿𝗲 𝘁𝗵𝗲 𝗱𝗮𝘁𝗮 𝘀𝗵𝗼𝘂𝗹𝗱 𝗴𝗼:</p>

<p>🏆 Best case: Already in a database (organized and queryable)</p>

<p>🤷 More common: Scattered across CSV files, scripts, documents</p>

<p>💡 Pragmatic solution: Organized cold storage</p>

<p>For smaller companies, S3 bucket works well:</p>

<p>→ Cheap long-term storage</p>

<p>→ Flexible (dump everything)</p>

<p>→ Easy to retrieve when needed</p>

<p>Downside: S3 is a digital junk drawer without organization. 🗃️</p>

<p>𝗠𝗮𝗸𝗶𝗻𝗴 𝗶𝘁 𝘄𝗼𝗿𝗸:</p>

<p>→ Consistent naming conventions</p>

<p>→ Clear folder structure</p>

<p>→ README files explaining contents</p>

<p>→ Metadata manifest listing all datasets</p>

<p>𝗧𝗵𝗲 𝗶𝗻𝘀𝗶𝗴𝗵𝘁:</p>

<p>Data archiving isn’t just storage—it’s knowledge preservation. Today’s “failed” experiment might be tomorrow’s breakthrough insight, but only if someone can understand what it was and why it mattered. 💡</p>

<p>𝗙𝗼𝗿 𝗹𝗲𝗮𝗱𝗲𝗿𝘀𝗵𝗶𝗽:</p>

<p>Build data sunset procedures into project workflows. The cost of storage is trivial compared to regenerating lost datasets. 💰</p>

<p>𝗧𝗵𝗲 𝗵𝗮𝗿𝗱 𝘁𝗿𝘂𝘁𝗵:</p>

<p>Most biotech companies are terrible at this. We’re great at generating data, mediocre at organizing it, awful at preserving institutional knowledge when projects end.</p>

<p>It doesn’t have to be this way. ✨</p>

<p>What’s your experience with data from discontinued projects? Have you seen companies do this well?</p>]]></content><author><name>lina</name></author><category term="data-science" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">The Engineering Report That Never Gets Written</title><link href="http://localhost:4000/software-engineering/2025/08/25/the-engineering-report-that-never-gets-written.html" rel="alternate" type="text/html" title="The Engineering Report That Never Gets Written" /><published>2025-08-25T09:00:00-04:00</published><updated>2025-08-25T09:00:00-04:00</updated><id>http://localhost:4000/software-engineering/2025/08/25/the-engineering-report-that-never-gets-written</id><content type="html" xml:base="http://localhost:4000/software-engineering/2025/08/25/the-engineering-report-that-never-gets-written.html"><![CDATA[<p><img src="/assets/images/posts/2025-08-25-the-engineering-report-that-never-gets-written.png" alt="Where do you store knowledge?" /></p>

<p>You just finished a three-month bioinformatics project. The analysis is done, results delivered, everyone moves on to the next urgent task.</p>

<p>But what about all the things you learned along the way?</p>

<p>Software engineers routinely write project wrap-up reports. Bioinformaticians? Almost never.</p>

<p>THE MISSING KNOWLEDGE:</p>

<p>➡️ Which approaches worked (and which didn’t)</p>

<p>➡️ What data quality issues you discovered</p>

<p>➡️ Where you spent the most debugging time</p>

<p>➡️ What you’d do differently knowing what you know now</p>

<p>All of that hard-won knowledge just… evaporates.</p>

<p>WHY WE DON’T DO IT: The honest reason? Time. Nobody wants to stop and document when there’s another analysis waiting. Leadership isn’t pushing for it.</p>

<p>THE BUSINESS CASE: Your company just invested weeks or months of your time on this project. What if the next person doing similar work could skip the dead-end approaches you already tried and build on your work instead of reinventing it?</p>

<p>Every repeated mistake is time stolen from innovation.</p>

<p>WHAT IT LOOKS LIKE: It doesn’t need to be formal. A Confluence page, slide deck, or simple document:</p>

<p>⭐ Project overview and key technical decisions</p>

<p>⭐ What worked vs. what didn’t</p>

<p>⭐ Lessons learned and recommendations</p>

<p>⭐ Useful resources discovered</p>

<p>MAKING IT HAPPEN:</p>

<p>✅ Build 2-3 hours of “project wrap-up” into every timeline</p>

<p>✅ Create a simple template</p>

<p>✅ Store reports where people can find them</p>

<p>✅ Make it part of project closure, not an afterthought</p>

<p>KEY INSIGHT: Engineering reports aren’t just documentation—they’re knowledge transfer tools. They help teams build on each other’s work instead of starting from scratch every time.</p>

<p>When people leave, does their knowledge walk out the door with them?
What’s your experience with project documentation? Have you seen teams successfully capture and share learnings?</p>]]></content><author><name>lina</name></author><category term="software-engineering" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">The Post-Mortem No One Wants to Do (But Everyone Should)</title><link href="http://localhost:4000/software-engineering/2025/08/21/the-post-mortem-no-one-wants-to-do.html" rel="alternate" type="text/html" title="The Post-Mortem No One Wants to Do (But Everyone Should)" /><published>2025-08-21T09:00:00-04:00</published><updated>2025-08-21T09:00:00-04:00</updated><id>http://localhost:4000/software-engineering/2025/08/21/the-post-mortem-no-one-wants-to-do</id><content type="html" xml:base="http://localhost:4000/software-engineering/2025/08/21/the-post-mortem-no-one-wants-to-do.html"><![CDATA[<p><img src="/assets/images/posts/2025-08-21-the-post-mortem-no-one-wants-to-do.png" alt="Continuous Learning Cycle." /></p>

<p>Something breaks. Data pipeline fails. Analysis crashes. Dashboard goes down.</p>

<p>Your first instinct? Fix it fast and move on. Nobody wants to dwell on what went wrong.</p>

<p>But here’s what I’ve learned: the 30 minutes you spend on a post-mortem can save you 30 hours of future firefighting.</p>

<p>THE NATURAL RESPONSE: When things break, we’re frustrated. We want to forget it happened and get back to “real work.” The failure feels like a setback, so we rush to put it behind us.</p>

<p>I get it. Post-mortems feel like dwelling on negative things when you could be building new features.</p>

<p>WHAT POST-MORTEMS ACTUALLY DO:</p>

<p>➡️ Identify systemic issues (not just the immediate bug)</p>

<p>➡️ Reveal process gaps you didn’t know existed</p>

<p>➡️ Prevent the same failure from happening again</p>

<p>➡️ Make your systems more robust by addressing root causes</p>

<p>➡️ Turn failures into learning opportunities for the whole team</p>

<p>A REAL EXAMPLE: Our visualization dashboard went down because a scientist uploaded a malformed CSV. Easy fix: validate the file format.</p>

<p>Post-mortem revealed the real issue: we had no systematic way to communicate data formatting requirements to scientists. The CSV was just the symptom.</p>

<p>Solution: Built an automated data validation step with clear error messages that taught scientists the expected format.</p>

<p>Result: Turned a one-off failure into a system improvement that prevented dozens of future issues.</p>

<p>THE PROCESS THAT WORKS:</p>

<p>1️⃣ Designate someone to lead the investigation (don’t let it fall through the cracks)</p>

<p>2️⃣ Focus on systems, not blame (what failed, not who failed)</p>

<p>3️⃣ Include relevant stakeholders (the people who were affected need to understand what happened)</p>

<p>4️⃣ Document actionable improvements (not just “be more careful next time”)</p>

<p>THE MANAGEMENT CHALLENGE: Here’s the tricky part: post-mortems require time that doesn’t immediately show ROI.</p>

<p>If leadership just wants to “move fast and fix things,” it’s hard to justify spending time on “what went wrong” instead of “what’s next.”</p>

<p>But here’s the business case: every failure that repeats is time stolen from innovation. Post-mortems are an investment in not having the same conversation again in three months.</p>

<p>THE BRIDGE BUILDER PERSPECTIVE: Post-mortems aren’t just technical exercises—they’re communication opportunities. They help teams understand how their work interconnects and where the fragile points are.</p>

<p>When you include stakeholders in post-mortems, you’re not just fixing systems—you’re building shared understanding of how your infrastructure works and why certain practices matter.</p>

<p>Have you found effective ways to make time for post-mortems? How do you convince leadership that this “backward-looking” work is actually forward-thinking?</p>]]></content><author><name>lina</name></author><category term="software-engineering" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">The One Question That Changed How I Build Tools</title><link href="http://localhost:4000/data-science/2025/08/18/the-one-question-that-changed-how-i-build-tools.html" rel="alternate" type="text/html" title="The One Question That Changed How I Build Tools" /><published>2025-08-18T09:00:00-04:00</published><updated>2025-08-18T09:00:00-04:00</updated><id>http://localhost:4000/data-science/2025/08/18/the-one-question-that-changed-how-i-build-tools</id><content type="html" xml:base="http://localhost:4000/data-science/2025/08/18/the-one-question-that-changed-how-i-build-tools.html"><![CDATA[<p><img src="/assets/images/posts/2025-08-18-the-one-question-that-changed-how-i-build-tools.png" alt="One research question can lead to different tool outcomes." /></p>

<p>“Can you make a dashboard for our RNA-seq data?”</p>

<p>I used to jump straight into requirements gathering. What data? Which visualizations? How many samples?</p>

<p>Now I ask one question first: “What does success look like?”</p>

<p>THE SAME REQUEST, DIFFERENT SUCCESS CRITERIA:</p>

<p>Scenario: “We need an RNA-seq dashboard”</p>

<p>⭐ If you’re the Lab Manager: Success = “I can quickly spot failed experiments before they waste downstream resources.” → Build: QC-focused dashboard with clear pass/fail indicators</p>

<p>⭐ If you’re the Principal Investigator: Success = “I can confidently present these results to the grant committee next week.” → Build: Publication-ready visualizations with statistical annotations</p>

<p>⭐ If you’re the Postdoc: Success = “I can explore the data myself without bothering the bioinformatics team every time I have a question.” → Build: Interactive exploration tool with multiple filtering options</p>

<p>Same request. Three completely different tools.</p>

<p>WHY REQUIREMENTS AREN’T ENOUGH: Requirements tell you WHAT to build. Success criteria tell you WHY you’re building it.</p>

<p>➡️ “Show differentially expressed genes” is a requirement.</p>

<p>➡️ “Help me identify the top 3 pathways to focus our next experiments on” is a success criterion.</p>

<p>The second one tells you that you need statistical significance, pathway enrichment, and probably some way to rank or prioritize results.</p>

<p>THE POWER OF STARTING WITH OUTCOMES: When you start with success criteria:</p>

<p>➡️ You build tools people actually use</p>

<p>➡️ You avoid feature creep (if it doesn’t serve the success criteria, it’s not essential)</p>

<p>➡️ You can make trade-offs confidently</p>

<p>➡️ You know when you’re done</p>

<p>A REAL EXAMPLE:</p>

<p>Scientist: “I need a way to visualize our compound screening data.”</p>

<p>Me: “What does success look like?”</p>

<p>Scientist: “I want to walk into Monday’s meeting and confidently say ‘these 5 compounds are worth pursuing’ and defend that decision.”</p>

<p>Suddenly I’m not building a generic visualization tool. I’m building a decision-support system with confidence intervals, statistical significance testing, and clear ranking criteria.</p>

<p>THE BRIDGE BUILDER INSIGHT: Different stakeholders define success differently, even for identical requests. Your job isn’t just to translate requirements – it’s to uncover and align success criteria.</p>

<p>Sometimes the real win is realizing that three different stakeholders need three different tools, not one “comprehensive” solution that satisfies nobody.</p>

<p>What’s your experience with this? Do you find that starting with outcomes changes what you build?</p>]]></content><author><name>lina</name></author><category term="data-science" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">The Hidden Bottleneck in AI-Driven Drug Design</title><link href="http://localhost:4000/biotech/2025/08/11/the-hidden-bottleneck-in-ai-driven-drug-design.html" rel="alternate" type="text/html" title="The Hidden Bottleneck in AI-Driven Drug Design" /><published>2025-08-11T09:00:00-04:00</published><updated>2025-08-11T09:00:00-04:00</updated><id>http://localhost:4000/biotech/2025/08/11/the-hidden-bottleneck-in-ai-driven-drug-design</id><content type="html" xml:base="http://localhost:4000/biotech/2025/08/11/the-hidden-bottleneck-in-ai-driven-drug-design.html"><![CDATA[<p><img src="/assets/images/posts/2025-08-11-the-hidden-bottleneck-in-ai-driven-drug-design.png" alt="AI is being held back by insufficient data pipelines." /></p>

<p>Everyone talks about AI algorithms in drug discovery. The real bottleneck? The data pipelines feeding them.</p>

<p>I spend my days upstream of AI initiatives, and I see the same pattern everywhere: brilliant algorithms starving on broken data infrastructure.</p>

<p>THE SPREADSHEET PROBLEM: Data lives in Excel files emailed between teams. Results shared via Slack attachments. “Final” datasets saved in SharePoint with links passed around like digital hot potatoes.</p>

<p>Sound familiar?</p>

<p>This isn’t just messy—it’s data integrity suicide.</p>

<p>WHAT BREAKS:</p>

<p>➡️ No provenance: Where did this data come from? Which version is current?</p>

<p>➡️ No lineage: How was this dataset processed? Can we reproduce it?</p>

<p>➡️ Human error everywhere: Copy-paste mistakes, version confusion, accidental overwrites</p>

<p>➡️ AI garbage in, garbage out: Your model is only as good as your training data</p>

<p>THE REAL CHALLENGE: You need a framework for where data can live that’s:</p>

<p>➡️ Extensible: Science moves faster than software. Your system needs to adapt.</p>

<p>➡️ Connected: Automated data flow from instruments → LIMS → analysis → notebooks</p>

<p>➡️ Traceable: Every data point has a story you can follow</p>

<p>WHAT THIS LOOKS LIKE: Instead of emailing Excel files, you have:</p>

<p>➡️ Instruments that automatically deposit data into structured systems</p>

<p>➡️ LIMS that captures experimental metadata and sample lineage</p>

<p>➡️ Automated pipelines that connect wet lab data to computational analysis</p>

<p>➡️ Lab notebooks that link directly to the data they reference</p>

<p>THE AI PAYOFF: When your data infrastructure is solid, AI initiatives actually work. Your models train on clean, well-documented data. You can trace every prediction back to its source. You can reproduce and validate results.</p>

<p>When it’s broken? Your data scientists spend 80% of their time hunting for data and questioning its quality.</p>

<p>REALITY CHECK: This is R&amp;D/early discovery perspective. Regulated environments have different (often stricter) requirements. But the principle remains: AI success starts with data infrastructure.</p>

<p>Most biotech companies are trying to solve the algorithm problem when they should be solving the data problem first.</p>

<p>Your AI is only as smart as the data pipeline feeding it. Fix the pipeline, unleash the potential.</p>

<p>What’s your experience with data infrastructure challenges in AI initiatives? Have you seen companies get this balance right?</p>]]></content><author><name>lina</name></author><category term="biotech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">The Software Engineering Principle No One Teaches In Bioinformatics</title><link href="http://localhost:4000/software-engineering/2025/08/07/the-sw-eng-principle-no-one-teaches-in-bioinfo.html" rel="alternate" type="text/html" title="The Software Engineering Principle No One Teaches In Bioinformatics" /><published>2025-08-07T09:00:00-04:00</published><updated>2025-08-07T09:00:00-04:00</updated><id>http://localhost:4000/software-engineering/2025/08/07/the-sw-eng-principle-no-one-teaches-in-bioinfo</id><content type="html" xml:base="http://localhost:4000/software-engineering/2025/08/07/the-sw-eng-principle-no-one-teaches-in-bioinfo.html"><![CDATA[<p><img src="/assets/images/posts/2025-08-07-the-sw-eng-principle-no-one-teaches-in-bioinfo.png" alt="Let's separate our concerns!" /></p>

<p>Separation of Concerns - it’s one of the most important concepts in software engineering, and somehow it never made it into my bioinformatics courses 🤔 .</p>

<p>I learned this the hard way when trying to scale prototype analyses into maintainable, production-ready tools.</p>

<p>THE PROTOTYPE TRAP: Your initial analysis script works perfectly. It reads data, cleans it, runs analysis, generates plots, and saves results. All in one beautiful 500-line Python script.</p>

<p>Then stakeholders ask: “Can you run this on different data?” “Can we change the visualization?” “What if we use a different algorithm?”</p>

<p>Suddenly, your elegant prototype becomes a maintenance nightmare 😵‍💫 .</p>

<p>WHAT IS SEPARATION OF CONCERNS? Simply put: each part of your code should have one job and do it well.</p>

<p>Instead of one script that does everything, you separate:</p>

<p>➡️ Data ingestion (reading files, databases)</p>

<p>➡️ Data processing (cleaning, transformation, QC)</p>

<p>➡️ Analysis logic (algorithms, statistics)</p>

<p>➡️ Visualization (plotting, reporting)</p>

<p>➡️ Output handling (saving results)</p>

<p>WHY THIS MATTERS: Need to swap your RNA-seq aligner? Easy—you only touch the analysis module. Your data cleaning logic works for multiple projects. You can test each component independently.</p>

<p>NEXTFLOW: SEPARATION OF CONCERNS IN ACTION Many bioinformaticians already use this principle! NextFlow/Snakemake/CWL/etc workflows are a great example:</p>

<p>➡️ Each process handles one specific task</p>

<p>➡️ Swap your aligner? Only modify that process—the rest stays unchanged</p>

<p>THE EDUCATION GAP: Most bioinformatics courses focus on algorithms and statistics (crucial!) but don’t explicitly teach these software engineering principles.</p>

<p>We learn sequence alignment but not why organizing code into modular, single-purpose components makes everything more maintainable.</p>

<p>FOR PRACTITIONERS: If your analysis scripts are becoming unmaintainable monsters, it might be time to refactor with separation of concerns in mind.</p>

<p>What software engineering principles do you wish you’d learned earlier in your bioinformatics career?</p>]]></content><author><name>lina</name></author><category term="software-engineering" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Why Every Biotech Needs a Data Steward</title><link href="http://localhost:4000/data-science/2025/08/04/why-every-biotech-needs-a-data-steward.html" rel="alternate" type="text/html" title="Why Every Biotech Needs a Data Steward" /><published>2025-08-04T13:00:00-04:00</published><updated>2025-08-04T13:00:00-04:00</updated><id>http://localhost:4000/data-science/2025/08/04/why-every-biotech-needs-a-data-steward</id><content type="html" xml:base="http://localhost:4000/data-science/2025/08/04/why-every-biotech-needs-a-data-steward.html"><![CDATA[<p><img src="/assets/images/posts/2025-08-04-why-every-biotech-needs-a-data-steward.png" alt="Data Archeolgy is too expensive" /></p>

<p>I’ve been doing “data stewardship” work for years without calling it that—and definitely without getting paid for it.</p>

<p>Setting up naming conventions, documenting data lineage, establishing quality checks, creating metadata standards. I did it because I knew it would save me headaches later when someone asked, “Can you reproduce that analysis from six months ago?”</p>

<p>But here’s the problem: this critical work is always treated as a “side project” that gets squeezed between “real” deliverables.</p>

<p>WHAT HAPPENS WITHOUT A DATA STEWARD:</p>

<p>➡️ Scientists can’t find the data they generated last quarter</p>

<p>➡️ “Quick analyses” take days because nobody knows which dataset is the “clean” version</p>

<p>➡️ Due diligence fails because data provenance is unclear</p>

<p>➡️ Team members leave and take institutional knowledge with them</p>

<p>➡️ The same data quality issues get discovered (and fixed) repeatedly</p>

<p>WHY DATA ENGINEERS AREN’T ENOUGH: Data engineers build pipelines. Data stewards govern what flows through them.</p>

<p>A data engineer might build an ETL process. A data steward ensures that process includes proper metadata capture, establishes what “clean” means for your organization, and creates the documentation that lets someone else maintain it.</p>

<p>WHAT A DATA STEWARD ACTUALLY DOES:</p>

<p>➡️ Establishes and enforces data standards across teams</p>

<p>➡️ Creates the metadata that makes data findable and trustworthy</p>

<p>➡️ Builds governance processes that scale with your organization</p>

<p>➡️ Serves as the bridge between data generators (scientists) and data consumers (everyone)</p>

<p>➡️ Prevents technical debt before it becomes a crisis</p>

<p>THE BUSINESS CASE: How much time does your team spend hunting for data, questioning data quality, or rebuilding analyses because the original isn’t reproducible?</p>

<p>If each scientist spends even 2 hours per week on “data archaeology,” that’s probably enough to justify a full-time steward.
Add in the risk mitigation (failed due diligence, regulatory compliance, patent disputes) and the ROI becomes obvious.</p>

<p>TO BIOTECH LEADERS: This isn’t a “nice to have” role. Data stewardship is infrastructure—invisible when it works, catastrophic when it doesn’t.</p>

<p>You wouldn’t run a lab without quality control processes. Don’t run your data operations without data governance.</p>

<p>The question isn’t whether you can afford a data steward. It’s whether you can afford not to have one.</p>

<p>What’s your experience with data governance in biotech? Have you seen companies invest in dedicated stewardship roles, or is this still “everyone’s job” (which usually means no one’s job)?</p>]]></content><author><name>lina</name></author><category term="data-science" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">The Power of Listening Across Teams</title><link href="http://localhost:4000/research/2025/07/31/the-power-of-listening-across-teams.html" rel="alternate" type="text/html" title="The Power of Listening Across Teams" /><published>2025-07-31T13:00:00-04:00</published><updated>2025-07-31T13:00:00-04:00</updated><id>http://localhost:4000/research/2025/07/31/the-power-of-listening-across-teams</id><content type="html" xml:base="http://localhost:4000/research/2025/07/31/the-power-of-listening-across-teams.html"><![CDATA[<p><img src="/assets/images/posts/2025-07-31-the-power-of-listening-across-teams.png" alt="Listen for what's beneath the surface" /></p>

<p>For most of my career, I’ve been the outsider.</p>

<p>The bioinformatician embedded in wetlab teams. The “computer person” in rooms full of bench scientists who’ve spent years mastering their craft.</p>

<p>I have valuable skills, but here’s what I learned: having technical expertise means nothing if you can’t connect with the people who need it.</p>

<p>➡️ THE MISTAKE I USED TO MAKE: Walking into meetings thinking, “I know exactly how to solve this with code.” Then wondering why my brilliant solutions gathered dust.</p>

<p>➡️ WHAT ACTUALLY WORKS: Listening first. Really listening.</p>

<p>When a scientist says “this analysis takes forever,” don’t immediately jump to “I can automate that.” Ask: “What part feels most frustrating? What would success look like to you?”</p>

<p>Sometimes “this takes forever” means “I don’t trust the results” or “I can’t explain this to my boss” or “I’m afraid I’m missing something important.”</p>

<p>The technical solution for each of those is completely different.</p>

<p>🌟 EMPATHY IS A TECHNICAL SKILL I’ve learned that dismissing someone’s pain points—even when they seem “solvable” to you—is the fastest way to lose credibility.</p>

<p>When a postdoc says “I’ve been struggling with this analysis for weeks,” don’t say “that’s easy, just use this tool.” Say “that sounds really frustrating. Walk me through what you’ve tried.”</p>

<p>Their expertise in their domain is as real as your expertise in yours. Respect that.</p>

<p>🚀 THE BREAKTHROUGH MOMENT: When you stop being the person who “fixes things” and become the person who “understands problems,” everything changes.</p>

<p>Scientists start coming to you with their real challenges, not just their surface requests. They trust you with their uncertainties, their experimental constraints, their career pressures.</p>

<p>That’s when you can build solutions that actually matter.</p>

<p>Being the outsider isn’t a disadvantage—it’s your superpower. You see patterns they can’t see, you bring tools they haven’t considered. But only if you earn the right to be heard by truly listening first.</p>

<p>What’s your experience working across different disciplines? How do you build trust when you’re the “outsider” on a team?</p>]]></content><author><name>lina</name></author><category term="research" /><summary type="html"><![CDATA[]]></summary></entry></feed>