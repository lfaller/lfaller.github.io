<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-07-21T16:26:03-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Lina L. Faller, PhD</title><subtitle>Welcome to my webpage</subtitle><entry><title type="html">Build vs Buy in Biotech</title><link href="http://localhost:4000/biotech/2025/07/21/build-vs-buy-in-biotech.html" rel="alternate" type="text/html" title="Build vs Buy in Biotech" /><published>2025-07-21T13:00:00-04:00</published><updated>2025-07-21T13:00:00-04:00</updated><id>http://localhost:4000/biotech/2025/07/21/build-vs-buy-in-biotech</id><content type="html" xml:base="http://localhost:4000/biotech/2025/07/21/build-vs-buy-in-biotech.html"><![CDATA[<p><img src="/assets/images/posts/2025-07-21-real-cost-of-buying.png" alt="How to budget for integration costs" /></p>

<p>Iâ€™ve lived on both sides of the â€œbuild vs buyâ€ equation in biotech, and honestly? Both extremes taught me expensive lessons.</p>

<p>THE â€œBUILD EVERYTHINGâ€ COMPANY: We built our own LIMS, lab automation software, workflow orchestratorsâ€”everything powered by cloud infrastructure. It worked, but the maintenance burden was real.</p>

<p>THE â€œBUY EVERYTHINGâ€ COMPANY: Management thought we could just purchase our way to efficiency. The budget ballooned fast, and we still needed internal expertise to make anything work together.</p>

<p>Plot twist: None of our shiny new tools could talk to each other without expensive â€œmanaged services.â€ And guess what? We still needed internal project managers to coordinate with those managed services.</p>

<p>Hereâ€™s what nobody puts in the budget: the hidden cost of integration ğŸ’¸ ğŸ’¸ ğŸ’¸</p>

<p>Buying a tool isnâ€™t buying a solutionâ€”itâ€™s buying a component that needs to fit into your ecosystem. That integration work? It still requires your people, your time, and your expertise.</p>

<p>The reality Iâ€™ve learned: healthy companies live in the middle. Build your core differentiators, buy your commodity functions, but ALWAYS budget for the glue that holds it together.</p>

<p>The most successful biotech teams Iâ€™ve seen ask different questions:</p>

<p>1ï¸âƒ£ â€œWhat gives us competitive advantage?â€ (Build this)</p>

<p>2ï¸âƒ£ â€œWhatâ€™s table stakes for the industry?â€ (Buy this, but budget for integration)</p>

<p>3ï¸âƒ£ â€œDo we have the expertise to maintain this long-term?â€ (Be honest here)</p>

<p>The â€œbuy everythingâ€ approach often comes from leadership who think technology problems can be solved with purchasing decisions. But integration, customization, and ongoing maintenance still require internal technical expertise.</p>

<p>You canâ€™t outsource your way out of needing to understand your own data infrastructure.</p>

<p>Whatâ€™s your experience with build vs buy in biotech? Where have you seen companies get this balance right (or wrong)?</p>]]></content><author><name></name></author><category term="biotech" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Code Review Culture in Research Labs</title><link href="http://localhost:4000/research/software-engineering/2025/07/18/code-review-culture-in-research-labs.html" rel="alternate" type="text/html" title="Code Review Culture in Research Labs" /><published>2025-07-18T13:00:00-04:00</published><updated>2025-07-18T13:00:00-04:00</updated><id>http://localhost:4000/research/software-engineering/2025/07/18/code-review-culture-in-research-labs</id><content type="html" xml:base="http://localhost:4000/research/software-engineering/2025/07/18/code-review-culture-in-research-labs.html"><![CDATA[<p><img src="/assets/images/posts/2025-07-18-volodymyr-dobrovolskyy-KrYbarbAx5s-unsplash.jpg" alt="Cat sitting in front of a laptop studying code" />
<em>Cats can do science, too! Source: <a href="https://lnkd.in/e56E35J7">Volodymyr Dobrovolskyy on Unsplash</a></em></p>

<p>Code review in research settings is a classic Goldilocks problem: too little and you risk irreproducible science, too much and you kill discovery momentum.</p>

<p>Iâ€™ve experienced this challenge across teams of all sizes, and the solutions are surprisingly different:</p>

<p>â¡ï¸ The Solo Bioinformatician Dilemma: Youâ€™re embedded in a wetlab team. Whoâ€™s your peer? The postdoc who knows R (while you code in Python)? The PI who coded in FORTRAN 20 years ago?</p>

<p>Honestly, Iâ€™m still figuring this one out. Maybe external code review partnerships? Monthly virtual code clubs? Iâ€™d love to hear how the community solves this.</p>

<p>â¡ï¸ The stretched small team (3 people): Everyoneâ€™s on different projects, everyoneâ€™s oversubscribed. Code review feels like a luxury you canâ€™t afford.
We tried it. It failed. The reality? When youâ€™re the only person who understands both the biology AND the pipeline, peer review becomes performative rather than protective.</p>

<p>â¡ï¸ The sweet spot (5-6 people): This was where peer review actually workedâ€”but only because management explicitly protected our time for it. Key insight: leadership has to VALUE code review, not just require it.</p>

<p>We established â€œreview debtâ€ as a real metric. If your reviews were backlogged, you couldnâ€™t start new features. Sounds harsh, but it worked.</p>

<p>Hereâ€™s what I learned: code review culture isnâ€™t just about catching bugs. Itâ€™s about knowledge sharing, preventing single points of failure, and building team standards.</p>

<p>But in research, speed often beats perfection. The trick is finding review practices that ADD velocity instead of killing it.</p>

<p>Maybe we need research-specific review standards? Maybe pair programming works better than async reviews? Maybe some analyses deserve different review rigor than production pipelines?</p>

<p>Whatâ€™s your experience with code review in research settings? How do you balance rigor with discovery speed? And solo bioinformaticiansâ€”how do you handle this challenge?</p>]]></content><author><name></name></author><category term="research" /><category term="software-engineering" /><summary type="html"><![CDATA[Cats can do science, too! Source: Volodymyr Dobrovolskyy on Unsplash]]></summary></entry><entry><title type="html">The Bioinformatics Triangle: Memory, Elegance, and Speed ğŸ§¬</title><link href="http://localhost:4000/data-science/research/2025/07/15/the-bioinformatics-triangle.html" rel="alternate" type="text/html" title="The Bioinformatics Triangle: Memory, Elegance, and Speed ğŸ§¬" /><published>2025-07-15T13:00:00-04:00</published><updated>2025-07-15T13:00:00-04:00</updated><id>http://localhost:4000/data-science/research/2025/07/15/the-bioinformatics-triangle</id><content type="html" xml:base="http://localhost:4000/data-science/research/2025/07/15/the-bioinformatics-triangle.html"><![CDATA[<p><img src="/assets/images/posts/2025-07-15-the-bioinformatics-triangle.png" alt="The bioinformatics triangle" /></p>

<p>Just had a fascinating discussion about generating all 64 possible codons in Python. Three approaches emerged:</p>

<p>1ï¸âƒ£ The Elegant Approach: Beautiful, concise, readableâ€¦ but materializes all 64 codons in memory</p>

<p>2ï¸âƒ£ The Memory-Efficient Approach: Constant memory usage, scales to millions of k-mers</p>

<p>3ï¸âƒ£ The Quick-and-Dirty Approach: Copy-paste ready, zero computation, maximum clarity</p>

<p>Hereâ€™s the thing: in bioinformatics, weâ€™re constantly juggling massive datasets (think whole genomes), complex algorithms (phylogenetic trees, alignment scoring), and tight deadlines (grant applications, paper submissions).</p>

<p>For 64 codons? Any approach works fine. For analyzing all 15-mers in the human genome? That elegant list comprehension will crash your laptop. ğŸ’¥</p>

<p>The real skill isnâ€™t picking the â€œrightâ€ approachâ€”itâ€™s knowing when each approach fits. Sometimes you need the generator for scalability. Sometimes you need the hardcoded list for reliability. Sometimes you need the elegant one-liner for a quick analysis.</p>

<p>Where do you fall on this spectrum? Are you team â€œpremature optimization is evilâ€ or team â€œmemory efficiency from day oneâ€? How do you balance code aesthetics with performance in your bioinformatics workflows?</p>]]></content><author><name></name></author><category term="data-science" /><category term="research" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">The Power of Strategic Data Infrastructure</title><link href="http://localhost:4000/data-science/biotech/2025/07/13/the-power_of_strategic_data_infrastructure.html" rel="alternate" type="text/html" title="The Power of Strategic Data Infrastructure" /><published>2025-07-13T13:00:00-04:00</published><updated>2025-07-13T13:00:00-04:00</updated><id>http://localhost:4000/data-science/biotech/2025/07/13/the-power_of_strategic_data_infrastructure</id><content type="html" xml:base="http://localhost:4000/data-science/biotech/2025/07/13/the-power_of_strategic_data_infrastructure.html"><![CDATA[<p>Sometimes the best features come from a single sentence in a meeting.</p>

<p>I was sitting with our wetlab director when she mentioned, almost in passing: â€œI wish we could see how our controls have performed historicallyâ€¦â€</p>

<p>My brain lit up. â€œWaitâ€”you want this data? I HAVE this data!â€</p>

<p>Because weâ€™d built our data warehouse to be comprehensive from day one, this â€œwishâ€ became reality in under a week. One new tab in our visualization app, and suddenly scientists could track control performance trends across months of experiments.</p>

<p>Hereâ€™s what made this magic possible:</p>

<p>â¡ï¸ Strategic infrastructure pays dividends: We didnâ€™t just build for todayâ€™s requirementsâ€”we captured everything we could think of, knowing future questions would emerge. That upfront investment in comprehensive data modeling meant we could pivot to new insights instantly.</p>

<p>â¡ï¸ Low effort, high impact: The technical lift was minimal because the foundation was solid. No emergency data migrations, no rushed ETL pipelines. Just a new query and some charts.</p>

<p>â¡ï¸ Collaboration creates breakthroughs: Hereâ€™s my favorite part: I never thought to surface this data on my own. It took a scientistâ€™s domain expertise to recognize the value hidden in our warehouse.</p>

<p>This is how great science happensâ€”when technical infrastructure meets scientific curiosity. The data was always there, but it took cross-functional conversation to unlock its potential.</p>

<p>The lesson? Build your data foundation wide and deep. You never know which â€œI wish we could seeâ€¦â€ will become your next game-changing feature.</p>

<p>And always, ALWAYS listen for those casual comments in meetings. Theyâ€™re often treasure maps to your next big win.</p>

<p>What unexpected insights have emerged from your data infrastructure? What â€œcasual wishesâ€ turned into powerful features?</p>]]></content><author><name></name></author><category term="data-science" /><category term="biotech" /><summary type="html"><![CDATA[Sometimes the best features come from a single sentence in a meeting.]]></summary></entry><entry><title type="html">Why Computational Biologists Need Lab Notebooks</title><link href="http://localhost:4000/research/2025/07/10/computational_biologists_need_lab_notebooks.html" rel="alternate" type="text/html" title="Why Computational Biologists Need Lab Notebooks" /><published>2025-07-10T13:00:00-04:00</published><updated>2025-07-10T13:00:00-04:00</updated><id>http://localhost:4000/research/2025/07/10/computational_biologists_need_lab_notebooks</id><content type="html" xml:base="http://localhost:4000/research/2025/07/10/computational_biologists_need_lab_notebooks.html"><![CDATA[<p>Biologists learn to keep lab notebooks in Bio 101. They document experimental designs, observations, what worked, what didnâ€™t.</p>

<p>But somehow, this fundamental practice gets lost when we move to computational biology.</p>

<p>Iâ€™ve met countless bioinformaticians and data scientists who can tell you the exact pH of their last buffer, but canâ€™t remember why they chose specific parameters for an analysis they ran last month.</p>

<p>Hereâ€™s why I think every computational biologist should keep a lab notebook:</p>

<p>â¡ï¸ SCENARIO: You run a complex command line tool with 15 parameters. Six months later, you need to reproduce the analysis on a similar dataset.</p>

<p>â¡ï¸ WITHOUT A NOTEBOOK: Youâ€™re digging through 10,000 lines of bash history, trying to remember if you used â€“min-coverage 10 or 20, and WHY you made that choice.</p>

<p>â¡ï¸ WITH A NOTEBOOK: â€œTried â€“min-coverage 10 initially but got too much noise in low-quality regions. Switched to 20 based on Smith et al. 2023 recommendation for similar tissue type.â€</p>

<p>The magic isnâ€™t just recording WHAT you didâ€”itâ€™s capturing WHY you did it.
When you document your rationale in real-time, youâ€™re not just helping future you. Youâ€™re building institutional knowledge that can be shared, reviewed, and improved upon.</p>

<p>Your notebook becomes a roadmap for scaling analyses, training team members, and catching edge cases before they become problems.</p>

<p>We wouldnâ€™t accept a wetlab scientist who couldnâ€™t reproduce their experiments. Why do we accept computational work that canâ€™t be reproduced?</p>

<p>The best part? Your â€œlab notebookâ€ can be as simple as a markdown file alongside your code. No fancy tools required. (although I personally am a Confluence fan girl ğŸ¤“ )</p>

<p>Do you keep a computational lab notebook? Whatâ€™s your system for documenting analysis decisions?</p>]]></content><author><name></name></author><category term="research" /><summary type="html"><![CDATA[Biologists learn to keep lab notebooks in Bio 101. They document experimental designs, observations, what worked, what didnâ€™t.]]></summary></entry><entry><title type="html">Enabling non-technical Users</title><link href="http://localhost:4000/data-science/2025/07/03/enabling-non-technical-users.html" rel="alternate" type="text/html" title="Enabling non-technical Users" /><published>2025-07-03T13:00:00-04:00</published><updated>2025-07-03T13:00:00-04:00</updated><id>http://localhost:4000/data-science/2025/07/03/enabling-non-technical-users</id><content type="html" xml:base="http://localhost:4000/data-science/2025/07/03/enabling-non-technical-users.html"><![CDATA[<p>After 10+ years of building tools for wetlab scientists, Iâ€™ve learned that making complex genomic data accessible isnâ€™t about dumbing it downâ€”itâ€™s about smart design.</p>

<p>Here are my 3 core principles:</p>

<ol>
  <li>Start with the Question, Not the Data</li>
</ol>

<p>Donâ€™t ask â€œWhat can we visualize?â€ 
Ask â€œWhat decision are you trying to make?â€</p>

<p>I once spent weeks building a beautiful multi-omics dashboard. Scientists used it twiceâ€¦ Why? It answered questions they werenâ€™t asking.</p>

<p>Now I start every project with a variation on: â€œWalk me through your Monday morning.</p>

<p>What would make you say â€˜Finally, I can see whatâ€™s happeningâ€™?â€</p>

<ol>
  <li>Make the Common Case Trivial</li>
</ol>

<p>80% of your users need 20% of your features. Make those 20% effortless.</p>

<p>Example: Scientists donâ€™t need to see every QC metricâ€”they need to know â€œIs this sample good enough to trust?â€ One green checkmark beats 15 charts.
Save the deep-dive complexity for the 20% who need it.</p>

<ol>
  <li>Show Impact, Not Just Data</li>
</ol>

<p>Raw numbers are intimidating. Context is powerful.</p>

<p>Instead of showing â€œExpression level: 2.3 FPKM,â€ show â€œ3x higher than baselineâ€ with a simple arrow. Instead of p-values, show â€œStrong evidence of difference.â€</p>

<p>Scientists are brilliantâ€”they donâ€™t need protection from complexity. They need their cognitive load focused on insights, not interpretation.</p>

<p>The goal isnâ€™t to make scientists into data analysts. Itâ€™s to make data analysis invisible so they can focus on being scientists.</p>

<p>Whatâ€™s your experience building tools for non-technical users? What works (or doesnâ€™t work) in your field?</p>]]></content><author><name></name></author><category term="data-science" /><summary type="html"><![CDATA[After 10+ years of building tools for wetlab scientists, Iâ€™ve learned that making complex genomic data accessible isnâ€™t about dumbing it downâ€”itâ€™s about smart design.]]></summary></entry><entry><title type="html">A Case for Data Democratization</title><link href="http://localhost:4000/data-science/2025/07/01/a-case-for-data-democratization.html" rel="alternate" type="text/html" title="A Case for Data Democratization" /><published>2025-07-01T13:00:00-04:00</published><updated>2025-07-01T13:00:00-04:00</updated><id>http://localhost:4000/data-science/2025/07/01/a-case-for-data-democratization</id><content type="html" xml:base="http://localhost:4000/data-science/2025/07/01/a-case-for-data-democratization.html"><![CDATA[<p>Thereâ€™s a critical tipping point every biotech company hits: your computational team canâ€™t keep up with the data your scientists produce.</p>

<p>Suddenly, the very team meant to accelerate discovery becomes the bottleneck. Scientists are waiting days for reports while breakthrough insights sit locked in databases.</p>

<p>This is when smart organizations invest in empowering their wetlab scientists with self-service tools. Why? Because science moves at breakneck speed, and handcrafted reports simply canâ€™t scale.</p>

<p>But hereâ€™s the nuance most companies miss: timing matters.</p>

<p>Early in discovery, when youâ€™re still prototyping analyses and figuring out whatâ€™s important, custom reports are essential. You canâ€™t standardize what you havenâ€™t defined yet.</p>

<p>The magic happens when you graduate from â€œWhat should we measure?â€ to â€œHow do we measure this consistently?â€ Thatâ€™s your cue to build tools that let scientists explore their own data.</p>

<p>Iâ€™ve seen teams reduce analysis turnaround from days to minutes by giving scientists intuitive dashboards (personally, I am a big fan of streamlit ğŸ¤“ but I am not going to say no to shiny). The computational team shifts from being report factories to building platforms that amplify scientific productivity.</p>

<p>The goal isnâ€™t to replace computational expertiseâ€”itâ€™s to democratize access so brilliant scientists can spend their time doing what they do best: making discoveries.</p>

<p>Whatâ€™s your experience with this tipping point? Have you seen teams successfully navigate this transition?</p>]]></content><author><name></name></author><category term="data-science" /><summary type="html"><![CDATA[Thereâ€™s a critical tipping point every biotech company hits: your computational team canâ€™t keep up with the data your scientists produce.]]></summary></entry></feed>